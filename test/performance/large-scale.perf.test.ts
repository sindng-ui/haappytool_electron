/**
 * Large Scale Performance Tests
 * 
 * âš ï¸ WARNING: ì´ í…ŒìŠ¤íŠ¸ëŠ” ì‹¤ì œ ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ìƒì„±í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.
 * - ì‹¤í–‰ ì‹œê°„: 5-10ë¶„ ì´ìƒ
 * - ë””ìŠ¤í¬ ê³µê°„: ìµœì†Œ 5GB í•„ìš”
 * - ë©”ëª¨ë¦¬: ìµœì†Œ 4GB ê¶Œì¥
 * 
 * ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  í…ŒìŠ¤íŠ¸ëŠ” .skipìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
 * ì‹¤í–‰í•˜ë ¤ë©´ .skipì„ ì œê±°í•˜ì„¸ìš”.
 */

import { describe, it, expect } from 'vitest';
import fs from 'fs';
import path from 'path';
import os from 'os';

const TEMP_DIR = path.join(os.tmpdir(), 'happytool-perf-test');
const LARGE_LOG_FILE = path.join(TEMP_DIR, 'large-test.log');

// 2GB ë¡œê·¸ íŒŒì¼ ìƒì„±ê¸°
async function generateLargeLogFile(sizeInGB: number): Promise<string> {
    console.log(`ğŸ”§ Generating ${sizeInGB}GB log file...`);

    if (!fs.existsSync(TEMP_DIR)) {
        fs.mkdirSync(TEMP_DIR, { recursive: true });
    }

    const targetSize = sizeInGB * 1024 * 1024 * 1024;
    const stream = fs.createWriteStream(LARGE_LOG_FILE, { flags: 'w' });

    let currentSize = 0;
    let lineNumber = 0;
    const startTime = Date.now();

    return new Promise((resolve, reject) => {
        const writeLines = () => {
            let canWrite = true;

            while (currentSize < targetSize && canWrite) {
                const timestamp = new Date().toISOString();
                const level = ['INFO', 'DEBUG', 'WARNING', 'ERROR'][lineNumber % 4];
                const line = `[${timestamp}] [${level}] This is log entry ${lineNumber++} with some additional text to increase file size.\n`;

                currentSize += line.length;
                canWrite = stream.write(line);

                // ì§„í–‰ë¥  í‘œì‹œ
                if (lineNumber % 100000 === 0) {
                    const progress = (currentSize / targetSize * 100).toFixed(2);
                    const elapsed = ((Date.now() - startTime) / 1000).toFixed(2);
                    console.log(`  Progress: ${progress}% (${(currentSize / 1024 / 1024).toFixed(2)}MB) - ${elapsed}s`);
                }
            }

            if (currentSize < targetSize) {
                stream.once('drain', writeLines);
            } else {
                stream.end(() => {
                    const duration = ((Date.now() - startTime) / 1000).toFixed(2);
                    console.log(`âœ… Generated ${(currentSize / 1024 / 1024 / 1024).toFixed(2)}GB in ${duration}s`);
                    resolve(LARGE_LOG_FILE);
                });
            }
        };

        writeLines();
        stream.on('error', reject);
    });
}

// ì²­í¬ ë‹¨ìœ„ íŒŒì¼ ì½ê¸° ë° ì²˜ë¦¬
async function processLargeLogInChunks(
    filePath: string,
    chunkSizeInMB: number,
    processor: (lines: string[]) => void
): Promise<{ totalLines: number; duration: number }> {
    const chunkSize = chunkSizeInMB * 1024 * 1024;
    const stream = fs.createReadStream(filePath, { encoding: 'utf8', highWaterMark: chunkSize });

    let buffer = '';
    let totalLines = 0;
    const startTime = Date.now();

    return new Promise((resolve, reject) => {
        stream.on('data', (chunk: string) => {
            buffer += chunk;
            const lines = buffer.split('\n');

            // ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ì¤„ì€ ë‹¤ìŒ ì²­í¬ë¡œ
            buffer = lines.pop() || '';

            totalLines += lines.length;
            processor(lines);

            if (totalLines % 100000 === 0) {
                console.log(`  Processed ${totalLines.toLocaleString()} lines...`);
            }
        });

        stream.on('end', () => {
            if (buffer) {
                const lines = [buffer];
                totalLines += lines.length;
                processor(lines);
            }

            const duration = Date.now() - startTime;
            resolve({ totalLines, duration });
        });

        stream.on('error', reject);
    });
}

describe('Large Scale Performance - Log Extractor (2GB+)', () => {
    // ì´ í…ŒìŠ¤íŠ¸ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ skipë˜ë©°, ëª…ì‹œì ìœ¼ë¡œ ì‹¤í–‰í•  ë•Œë§Œ ë™ì‘
    it.skip('should process 2GB log file efficiently', async () => {
        // 1. íŒŒì¼ ìƒì„±
        const logFile = await generateLargeLogFile(2);

        // 2. ì „ì²´ íŒŒì‹± í…ŒìŠ¤íŠ¸
        console.log('\nğŸ“Š Testing full file parsing...');
        const parseResult = await processLargeLogInChunks(logFile, 10, (lines) => {
            // ê° ì²­í¬ íŒŒì‹±
        });

        console.log(`  âœ… Parsed ${parseResult.totalLines.toLocaleString()} lines in ${(parseResult.duration / 1000).toFixed(2)}s`);
        console.log(`  ğŸ“ˆ Speed: ${(parseResult.totalLines / (parseResult.duration / 1000)).toFixed(0)} lines/sec`);

        expect(parseResult.totalLines).toBeGreaterThan(10_000_000); // ìµœì†Œ 1ì²œë§Œ ì¤„
        expect(parseResult.duration).toBeLessThan(120_000); // 2ë¶„ ì´ë‚´

        // 3. í•„í„°ë§ í…ŒìŠ¤íŠ¸
        console.log('\nğŸ“Š Testing filtering...');
        let errorCount = 0;
        const filterResult = await processLargeLogInChunks(logFile, 10, (lines) => {
            errorCount += lines.filter(line => line.includes('ERROR')).length;
        });

        console.log(`  âœ… Found ${errorCount.toLocaleString()} ERROR entries`);
        expect(errorCount).toBeGreaterThan(0);

        // 4. ì •ë¦¬
        fs.unlinkSync(logFile);
        console.log('ğŸ—‘ï¸  Cleaned up test file');
    }, 180000); // 3ë¶„ íƒ€ì„ì•„ì›ƒ

    it.skip('should handle memory efficiently with 2GB file', async () => {
        const logFile = await generateLargeLogFile(2);

        const getMemory = () => {
            if ((performance as any).memory) {
                return (performance as any).memory.usedJSHeapSize / 1024 / 1024;
            }
            return 0;
        };

        const memBefore = getMemory();
        console.log(`ğŸ“Š Memory before: ${memBefore.toFixed(2)}MB`);

        // ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        await processLargeLogInChunks(logFile, 10, (lines) => {
            // ì²˜ë¦¬ë§Œ í•˜ê³  ì €ì¥í•˜ì§€ ì•ŠìŒ
        });

        const memAfter = getMemory();
        const memIncrease = memAfter - memBefore;

        console.log(`ğŸ“Š Memory after: ${memAfter.toFixed(2)}MB`);
        console.log(`ğŸ“Š Memory increase: ${memIncrease.toFixed(2)}MB`);

        // ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì´ë¯€ë¡œ ë©”ëª¨ë¦¬ ì¦ê°€ê°€ í¬ì§€ ì•Šì•„ì•¼ í•¨
        if (memBefore > 0) {
            expect(memIncrease).toBeLessThan(500); // 500MB ì´ë‚´
        }

        fs.unlinkSync(logFile);
    }, 180000);
});

describe('Large Scale Performance - JSON Tools (1GB+)', () => {
    it.skip('should parse 1GB JSON efficiently', () => {
        // 1GB JSON ìƒì„±
        console.log('ğŸ”§ Generating 1GB JSON...');
        const largeData = {
            items: Array.from({ length: 1_000_000 }, (_, i) => ({
                id: i,
                name: `Item ${i}`,
                description: 'This is a sample item with some text to increase size. '.repeat(10),
                metadata: {
                    tags: ['tag1', 'tag2', 'tag3'],
                    timestamp: new Date().toISOString(),
                }
            }))
        };

        // ì§ë ¬í™”
        console.log('ğŸ“Š Testing stringify...');
        const startStringify = Date.now();
        const jsonString = JSON.stringify(largeData);
        const stringifyDuration = Date.now() - startStringify;

        const sizeInMB = jsonString.length / 1024 / 1024;
        console.log(`  âœ… Stringified ${sizeInMB.toFixed(2)}MB in ${stringifyDuration.toFixed(2)}ms`);

        // íŒŒì‹±
        console.log('ğŸ“Š Testing parse...');
        const startParse = Date.now();
        const parsed = JSON.parse(jsonString);
        const parseDuration = Date.now() - startParse;

        console.log(`  âœ… Parsed ${sizeInMB.toFixed(2)}MB in ${parseDuration.toFixed(2)}ms`);

        expect(parsed.items.length).toBe(1_000_000);
        expect(parseDuration).toBeLessThan(10_000); // 10ì´ˆ ì´ë‚´
    }, 60000); // 1ë¶„ íƒ€ì„ì•„ì›ƒ
});
